\chapter{Results}
In this work, considering the available flexibility along the various axes in the analysis 
(Section~\ref{sec:modelflex}), multiple models were built on the AAC (amino acid composition) 
feature set (Tables~\ref{tab:prob_7class},~\ref{tab:prob_8class},~\ref{tab:scikit_pred}). 
For each model, the distance between the generated results and the reference values from 
the original study was then calculated (Figure~\ref{fig:modelPerformance}). We then picked 
the settings from the 16 models showing the least distance from the reference values. 
The model settings from those 16 models were then used to build models for the 18 
remaining feature sets (Tables~\ref{tab:pssmAaindex},~\ref{tab:dpcPhcAaindexPssm},
~\ref{tab:aacDpcPhcAaindexPssm},~\ref{tab:dpcPhcAaindexPssmAaindexPhc},
~\ref{tab:aacDpcAaindexPssmHybrid3},~\ref{tab:aacAaindexPhcPssmHybrid3}). 
The results from all the 19 feature sets were then compared and 
illustrated in Figure~\ref{fig:MccAllModels}. Through this section, the context is also 
organized accordingly.

\section{AAC Models}
Tables~\ref{tab:prob_7class} contains the results of the probability-based models with 
7-classes of proteins in the dataset. All the models were evaluated using the performance 
metrics in in~\cite{mishra_prediction_2014}. Among all the models, 8 highlighted ones reported 
the distance values between $0.07$ and $0.13$ from the reference. These are the ones that produced 
the closest results to the initial ones.

Tables~\ref{tab:prob_8class} shows the performance of the probability-based models with 
8-classes of proteins in the dataset. Among all the models, the closest ones (8 highlighted models) 
reported distance values between $0.08$ and $0.10$ from the reference values. 

Table~\ref{tab:scikit_pred} shows the results of the scikit-learn prediction-based models 
for the models with 7- and 8-classes of proteins in the dataset. This function from the 
scikit-learn library, by default, aggregates the results using the maximum probability 
technique. Using this approach, all the models reported distance values above $0.32$ 
from the reference values.


\section{Closest Models}
Figure~\ref{fig:modelPerformance} shows the sensitivity 
and specificity of each tested model alongside the performance of the originally published model (on AAC feature set), 
with 10\% of models most closely matching performance to the reference highlighted.

The closest 10\% of models (16) used a variety of configurations, and each reported a distance score of less than
$0.13$ from the reference. The breakdown of configurations for these models included: micro aggregation (all), balanced
average prediction method (all), balanced (8) or shuffled (8) dataset, contained 7 (8) or 8 (8) classes in the dataset,
were trained with uniform (8) or heterogeneous (8) hyperparameters, and were developed using SVMLight (8) or the
Scikit-Learn Probability (8) model architectures. While the Scikit-Learn Prediction model and downsampled dataset configuration
are notably absent from these models, all other settings were either dominated by a single value, such as in the case
of micro aggregation and the balanced average prediction method, or the settings were equally represented. This
uniformity in representation is consistent with the direct comparisons between settings described above.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.90\textwidth]{figures/14ModelPerformance.png}
    \caption{Sensitivity and Specificity of each tested model. Each panel contains models 
    trained with a fixed number of categories (7: left; 8: right), and shows the published 
    reference performance in red. The closest 10\% of models to this reference have been 
    outlined in black. The symbol colour and shape refer to the classifier type and aggregation 
    strategy, respectively. Each shaded region illustrates the bounds of performance for a given 
    binary classifier aggregation strategy.}
    \label{fig:modelPerformance}
\end{figure}

\section{Model Differences}
This section will explore the differences in the model performances based on the defined axes of
flexibility enumerated in Section~\ref{sec:experimentaldesign}.

\paragraph{Number of Classes}
While the 7-class models appear to be slightly closer to the reference, there was no significant difference between
the number of classes and the distance from reference ($p > 0.1$). Models trained with 8 classes tended to achieve
higher sensitivity and specificity values. It seems that the addition of the background class has improve the 
performance.

\paragraph{Dataset Sampling}
The dataset composition had no significant impact on the closeness of the model to the reference ($p > 0.1$ for all
comparisons). However, none of the closest 10\% of models were trained using the downsampled dataset.

\paragraph{SVM Hyperparameters}
All uniformly parameterized models converged to same set of hyperparameters within the number of classes. For the 
models with 7-classes of proteins, the ones with the closest performance used Gamma and Cost 
values of $0.02$ and $4.5$ while for the models with 8-classes of proteins, the closest results were achieved using 
the values of $0.01$ and $4$ for Gamma and Cost respectively. There was no significant difference between these 
sets of parameters.

\paragraph{Hypterparamter Heterogeneity}
Similarly to the case of uniform parameters, models converged on Gamma values between $0.02$ and $0.04$ for all classes
and models, and Cost values between $4$ and $5$, with no statistically significant difference between models or classes.

\paragraph{Aggregation Technique}
Models using the micro performance-aggregation technique (i.e. evaluating individual binary classifiers prior to
aggregation into a multi-class classifier) obtained closer results to the reference than those using the macro
technique ($p < 1\times 10^{-4}$). All of the closest models used micro-aggregation.

\paragraph{Prediction Method}
The balanced averaging prediction method produced significantly closer results to the reference than both the
unweighted average and maximum probability methods ($p < 1\times 10^{-5}$ for both). The maximum probability method
also produced significantly closer results than the unweighted average method ($p < 0.001$).

\paragraph{Tool}
The SVMLight classifiers produced closer results to the reference than both Scikit-Learn Probability and Prediction models
($p < 0.05$ for both). While the Scikit-Learn Prediction model architecture did not appear in the set of closest models,
there was no statistically significant difference between its performance and that of the Scikit-Learn Probability models.

\section{All Feature Sets Results}
According to~\cite{mishra_prediction_2014}, among all the 19 feature sets, the hybrid dataset 
that includes the biochemical composition (AAindex) and the PSSM profile, provides the best 
results for the membrane protein classification with the highest MCC value.

Table~\ref{tab:pssmAaindex} shows the results from running the closest-performing models 
(from the AAC experiment) on the main and the independent datasets for the AAindex+PSSM 
profile dataset. All 16 models reported distance values between $0.07$ and $0.09$ from 
the reference values. 

Tables~\ref{tab:dpcPhcAaindexPssm},~\ref{tab:aacDpcPhcAaindexPssm},
~\ref{tab:dpcPhcAaindexPssmAaindexPhc},~\ref{tab:aacDpcAaindexPssmHybrid3},
~\ref{tab:aacAaindexPhcPssmHybrid3} 
contain the results from running the 10\% closest-performing models (16) on all the other 
18 features.

Table~\ref{tab:dpcPhcAaindexPssm} shows the results from running those models on DPC, 
PHC, AAindex and PSSM feature sets. They all reported the distance values 
between $0.06$ and $0.13$ from the reference values. For the DPC feature set, the models 
seem to perform slightly better in the 7-class-based settings. The rest of the models 
(for the PHC, AAindex and PSSM feature sets) reported quite close performance values 
in both 7- and 8-class-based settings.

Tables~\ref{tab:aacDpcPhcAaindexPssm},~\ref{tab:dpcPhcAaindexPssmAaindexPhc} show the results 
from running the closest modelsâ€™ settings on the hybrid feature sets being produced by 
combining 2 different features (8 features). All the models reported distance values between 
$0.06$ and $0.13$ from the reference values. The models for DPC+AAC, DPC+PSSM and DPC+AAINDEX 
feature sets seem to perform slightly better in 7-class-based settings while the results 
from all the other 5 feature sets show a close performance in both 7- and 8-class-based settings.

Tables~\ref{tab:aacDpcAaindexPssmHybrid3},~\ref{tab:aacAaindexPhcPssmHybrid3} show the results 
from running the closest models' setting on the hybrid datasets being produced by combining 3 
feature sets(6 features). All the models reported distance values between $0.06$ and $0.11$ 
from the reference values. The hybrid AAC+DPC+AAINDEX model seems to perform slightly 
better in 7-class-based settings while all the models show a close performance in 
both 7- and 8-class-based settings.

Figure~\ref{fig:MccAllModels} compares the MCC values resulting from running the 
closest-performing models on all the feature sets (19 features). The hybrid dataset 
that includes the biochemical composition (AAindex) and the PSSM profile, outperforms others. 
Compared to all the other models, these models produce the highest MCC values. 

%Tables
\input{tables/1aac7Probability.tex}
\input{tables/2aac8Probability.tex}
\input{tables/3aacprediction.tex}
\input{tables/4aaindexPssm.tex}
\input{tables/5dpcPhcAaindexPssm.tex}
\input{tables/6aacDpcPhcAaindexPssm.tex}
\input{tables/7dpcPhcAaindexPssmAaindexPhc.tex}
\input{tables/8aacDpcAaindexPssmHybrid3.tex}
\input{tables/9aacAaindexPhcPssmHybrid3.tex}
\begin{figure}[ht]
    \centering
    \includegraphics[width=14cm,height=21cm]{figures/15MccAllModels.png}
    \caption{MCC results from applying the closest 10\% models to all the features.
    The hybrid model that included the AAindex and the PSSM profile (7th box), outperforms others.}
    \label{fig:MccAllModels}
\end{figure}
