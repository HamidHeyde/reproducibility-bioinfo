\section {Results}
\label{sec:results}

The classifers developed in this replication attempt were constructed and evaluated according to the parameters
and metrics described in Section~\ref{sec:modelflex}. Figure~\ref{fig:7_class_model} shows the sensitivity
and specificity of each tested model alongside the performance of the originally published model, with 10\% of models
most closely matching performance to the reference highlighted.
Tables~\ref{tab:prob_7class} and~\ref{tab:prob_8class} contain the complete performance of the probability-based models
evaluated using each performance metric for the 7- and 8-class settings, respectively. Table~\ref{tab:scikit_pred}
shows the same for the SKlearn Prediction models.

Finally, Table~\ref{tab:all_feature_performance} shows the evaluation
of the best-performing models fit on AAC when applied across all features.
\GK{For Hamid: regenerate data in the all-feature-performance table based on more than just the "best" model. At
present, we cannot include anything in the results about this because there isn't a result to discuss.}


The remainder of this section will explore the differences in model performance based on the defined axes of
flexibility enumerated in Section~\ref{sec:experimentaldesign}.

% Figure 1
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/fig1_model_performance.pdf}
  \caption{Sensitivity and Specificity of each tested model. Each panel contains models trained with a fixed number of
categories (7: left; 8: right), and shows the published reference performance in red. The closest 10\% of models to
this reference have been outlined in black. The symbol colour and shape refer to the classifier type and aggregation
strategy, respectively. Each shaded region illustrates the bounds of performance for a given aggregation strategy.}
   \label{fig:7_class_model}
\end{figure}

\paragraph{Number of Classes}
While the 7-class models appear to be slightly closer to the reference, there was no significant difference between
the number of classes and the distance from reference ($p > 0.1$). Models trained with 8 classes tended to achieve
higher sensitivity and specificity values, however, suggested improved performance with the addition of the background
class. 

\paragraph{Dataset Sampling}
The dataset composition had no significant impact on the closeness of the model to the reference ($p > 0.1$ for all
comparisons).

\paragraph{SVM Hyperparameters}
All uniformly parameterized models converged to same set of hyperparameters within the number of classes, where the
best performing 7-class models used Gamma and Cost values of $0.02$ and $4.5$ while 8-class models had best performance
with $0.01$ and $4$, respectively. There was no significant difference between these sets of parameters.

\paragraph{Hypterparamter Heterogeneity}
Similarly to the case of uniform parameters, models converged on Gamma values between $0.02$ and $0.04$ for all classes
and models, and Cost values between $4$ and $5$, with no statistically significant difference between models or classes.

\paragraph{Aggregation Technique}
Models using the micro performance-aggregation technique (i.e. evaluating individual binary classifiers prior to
aggregation into a multi-class classifier) obtained closer results to the reference than those using the macro
technique ($p < 1\times 10^{-4}$). All of the closest models used micro-aggregation.

\paragraph{Prediction Method}
balanced closer than unweighted (p < 1e-5)
maximum prob closer than unweighted (p < 1e-2)
balanced closer than maximum prob (p < 1e-5) 

Regrading the different aggregation techniques, the maximum probability models corresponds to multi-class classification 
problems where there is one predicted label for each element in the dataset. For the problems in this category, 
the results from all 3 software programs are very close through the same settings. We suggest the ScikitLearn predictor 
since it involves less coding and thus provides better reproducibility when merged with other tools from the ScikitLearn Library.

Overall for both SvmLight and ScikitLearn binary classifiers, the closest results to the original ones are achieved through
balanced averaging aggregation technique on shuffled and balanced datasets when the metrics are Micro averaged.

\paragraph{Tool}
SVMLight closer than SKlearn prob (p < 0.005)
SVMLight closer than SKlearn pred (p < 0.05)

\subsection{Closest Models}
The closest 10\% of models (16) used: micro aggregation (all), balanced average prediction method (all), the balanced (8)
or shuffled (8) dataset, contained 7 (8) or 8 (8) classes in the dataset, were trained with uniform (8) or
heterogeneous (8) hyperparameters, and were developed using SVMLight (8) or the SKlearn Probability (8) model
architectures.


% Table 1
\input{tables/tab_7_class_prob_performance.tex}
\input{tables/tab_8_class_prob_performance.tex}
\input{tables/tab_scikit_pred_performance.tex}
\input{tables/tab_best_model_multifeature.tex}
