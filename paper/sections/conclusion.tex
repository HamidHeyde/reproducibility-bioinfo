\section {Conclusion}

\tbm{
    add links and references to the good practices that we did for this paper in each section and 
    for each point in the table
}

Through this work, we conducted a reproducibility study on a machine learning classification problem dealing with multiple imbalanced 
classes of data in Bioinformatics to observe and address missing information that could affect replication process through 
different phases of common machine learning problem. The research involves various techniques from data curation to model evaluation, 
using an independent software for running Support Vector Machine (SVM) algorithm on the data, results aggregation and addressing 
sharing concerns involved with output from each phase.\\

Through Materials and Methods section, we addressed all the missing information that affects the replication process and the results (in 
regards to provided information). In the next section, we discussed possible resources through which these sort of problems 
could be handled when dealing with reproducibility. Through this work, we tried to implement our concerns in a project providing 
a tangible reference for what we discussed in this paper.\\ 

In this section, we provide general guidelines by which we believe one can create a reproducible work when dealing with such a problem. 
According to W3C Incubator Group Report, provenance of a resource is a record that describes entities and processes involved in producing 
and delivering or otherwise influencing that resource. Provenance provides a critical foundation for assessing authenticity, enabling trust, and allowing reproducibility.\cite{w3c} So, for addressing the human-readable provenance problems (as opposed to machine-readable provenances), 
in this study, we organized our proposed guidelines under data provenance, feature provenance, model provenance, software provenance 
and pipeline provenance. We also provided a table containing all the notes from each category along with a sample for each point (where possible).\\ 

\subsection{Data Provenance and Sharing}


    Data Provenance is a record that describes entities and processes involved in producing and delivering or otherwise influencing 
    that resource.\cite{w3c}  In other words, all the details and processes involved in putting together a dataset from the raw data. 
    We should make sure to report on the source(s) where the data (i.e. protein sequences) is taken from and the process(es) 
    by which the final data for the dataset was produced (from the original data). \\
    
    If the study involves working on dataset of sequences, then, note that a sequence could get updated several times through years. 
    So, for each sequence, the details should include the database it has been taken from (i.e. UniProt), 
    the accession number and the version. Also if any software has been involved in the curation
    (the organization and integration of data collected from various sources) process, 
    the details should be reported as mentioned in software provenance section (\ref{sec:softwareProvenance})\\
    
    
    Sharing the dataset through available means would be a better practice as it saves significant amount of time when
    trying to replicate the whole experiment.
    Providing details over the curation process along with sharing the dataset would further allow the scientists to contribute to the 
    dataset by adding new sequences. The more sequences available for a problem, there would be a greater chance to develop a model 
    that could improve the stability and accuracy of the results.
    
\subsection{Feature Provenance and Sharing}

    Feature provenance refers to the historical record of how a feature is generated from the dataset. For each feature, explain 
    over the feature you are trying to extract from the dataset (i.e. Di-peptide Composition), the process details (through which the 
    feature is generated) and expected result from this process. For formulas, a clear description of the involved parameters should be 
    included in the report. If anywhere throughout the whole process, use of a random function is required, random function seed along 
    with the library details should be included. 
    Any involved coded program, software or environment related parameters should be reported as mentioned in 
    (\ref{sec:softwareProvenance}). Sharing the generated feature in any format is strongly advised.\\
    
    Describing the feature generation process along with sharing the final feature file or some certain rows from that feature ( a row 
    in a feature contains all the generated numerical values for a single input (i.e. sequence)) or providing a reference to any available 
    resource(s) (online or downloadable programs) that could produce the same numerical values, 
    would be the preferable practice as this would allow tuning the replicated pipeline to the state in which the generated results 
    would match the original ones. \\
    
    Note that some tools (being used throughout the project pipeline at the time of the experiment) might become unavailable through years. 
    Following this practice allows use of any alternative tool available at the time of the study replication (if the generated results are 
    the same as the ones being reported on the original report). Sharing the extracted feature also enables researchers to update the existing 
    program (if needed) as the updated codes could be evaluated by comparing the new generated results against the original ones.
    
\subsection{Model Provenance and Sharing}

    Model provenance refers to the historical record of how the model is trained and evaluated. This includes any possible 
    data transformation (i.e. standardization), any applied feature selection or feature engineering technique, choice of 
    the algorithm along with its hyper-parameters, the trained model and the evaluation metrics.\\
    
    Throughout the data pre-processing phase, various data transformation techniques (i.e. Standardization, Normalization, etc.)
    could be applied to the original data available in the dataset. If that is the case, then the technique 
    (i.e. Standardization (Z-score scaling)) along with any involved parameters 
    (i.e. variables calculated as (V - mean of V)/s, where "s" is the standard deviation)
    should be clearly explained. The description over how it has been applied to the original data  and the expected result 
    should be also included.\\
    
    Depending on the type of study, dealing with problems such as missing data, data with high dimensionality or imbalanced classes 
    (where there are a disproportionate ratio of observations in each class) could be the case that demands for further various techniques 
    (such as feature selection, feature engineering, imputation, etc.) to be applied to the problem. In that case, explain the applied method 
    (i.e. ensemble feature selection) along with correspondent details and parameters (i.e. explain over what type of ensemble feature selection 
    has been used including all the details). Applying the method to the original dataset and the output from this phase should also be 
    discussed in details.\\
    
    
    Before fitting an algorithm, you need to split the data into train, validation and test subsets. Depending on the amount of 
    available data to the work and the goal of the study, The sampling and allocation of of those sampled data to 
    those three subsets above could be done through different techniques. Also, applying cross validation to the feature set, is a common 
    practice for this phase. For this part, the description of the technique, the process, how it has been applied to the data, along with 
    any involved parameter(s) and the expected result(s) should be included in the report.\\
    
    
    For the model, a clear explanation over the choice of the algorithm along with its considered range of hyper-parameters and 
    the associated values for producing the 
    the published results should be included. In case of involvement of any technique for finding the optimal hyper-parameters for the model
    (i.e. grid search), the description of the method, the process, how it has been applied to the project's data along with any involved 
    parameter(s) should be included. 
    If dealing with multiple classes, describe the process and how the results are aggregated.
    If the model is an ensemble of sub-models, then the model structure, the included underlying models and the aggregation 
    strategy should be clearly explained. Also, for each model, the details mentioned above should be included.\\
    

    Depending on the problem type and the purpose of the work, various metrics could be used to evaluate the results. 
    A clear definition of the statistical method used for results' evaluation should be included. 
    When a numerical metric (i.e. accuracy) is calculated by averaging through multiple results, 
    then explain over the choice of averaging method (micro versus macro) and 
    any possible details involved with process. Also, Error bars should be clearly defined and reported (if any).\\
  
    
    If a random function is used anywhere throughout the whole process, then, random function seed along 
    with the library details should be included. Any involved coded program, software or environment related parameters 
    should be reported as mentioned in (\ref{sec:softwareProvenance}). Sharing the final transformed feature set is strongly advised.
    
\subsection{Software Provenance and Sharing}
    \label{sec:softwareProvenance}

    From the data curation (the organization and integration of data collected from various sources)
    and putting together a dataset, to model training and evaluation, different software and coded programs 
    could get involved in some phase of a typical Bioinformatics related problem. 
    Throughout the replication process, failing to use the same software (along with its parameters 
    being set to the same state or value at the time of the experiment) through the same environment, can also affect the final results. \\
    
    The report on this aspect should cover all the software, coded programs, libraries details being involved throughout the whole process. 
    For software, name, version, installation details along with documentation should be reported.
    For coded programs, programming language, version, libraries and any required deployment instructions should be described.
    If the pipeline (\ref{sec:PipelineProvenance}) is environment-dependent, then, It should also include the environment-related 
    details in which the pipeline has been executed.\\

    A good practice (if applicable to the project) is including a "requirement.txt" file in your repository when sharing a project. 
    Working on machine learning related studies, it’s common that you get a lot of libraries installed. 
    Following the instructions (available through documentation \cite{pipDoc}) you can export all the 
    installed packages and dependencies along with the current version in use into a file (Commonly named as "requirement.txt") in a state when
    the results are obtained.
    

\subsection{Pipeline Provenance and Sharing}
    \label{sec:PipelineProvenance}
    
    In software engineering, a pipeline is a chain of processing modules, in a way that, the output from each module is the 
    input of the next. The pipeline should be designed and coded on a modular-basis. For example, if you have a process for data cleaning, 
    feature selection or feature extraction, each one of those should be coded as an independent runnable module. The pipeline is then formed 
    by chaining all the required modules for the problem. Following this practice, if one runs through some difficulties during the replication process, 
    then there would be a greater chance for tracing and recovering from a problem associated with one piece instead of the 
    whole pipeline.\\
    
    Changing direction in an experiment or setting new targets for the study (while working on the same problem) is a common phenomenon that 
    can take place throughout any research. So, in General the entire pipeline (including all chained modules) should be reported, 
    version-controlled and shared through available means (i.e GitHub). For each module, the input to the piece, expected output, 
    the module description and any possible associated parameters should be reported. 
    Creating a walk-through guidelines for the pipeline is considered a better practice as it would walk the reader through the entire project 
    providing a brief explanation over how the whole process works.\\
    
    The common popular practice for this purpose is Notebook creation and sharing. Using applications like "Jupyter Notebook", one can create a 
    document containing live code, equations, visualizations and narrative text to walk the reader through the pipeline starting from 
    the feature extraction all the way to result generation.\\
    
    For each step, explain briefly over what the module does (could be also referenced to the correspondent part in the paper for further details) 
    followed by a runnable code cell demonstrating how to run the code. 
    If running the entire code for a piece (i.e. feature selection) is not feasible, then leave the runnable code cell there followed by 
    explanation over the matter. If this is the case, then, provide details over the expected output from this process 
    (if available, you can load and display the process result instead).\\
    
    Another alternative solution to this problem is the use of containers \footnote{OS-level virtualization is a technology through which various virtual 
    environments could be deployed on top of a shared operating system. In this type of virtualization, the underlying operating system is partitioned
    to create multiple isolated Virtual Machines (VM) providing the functionality of a physical computer system. \cite{osLevelVM}} (i.e. Docker) 
    or virtual machines \footnote{A virtual machine (VM) is an emulated computer system. Based on the computer architectures, 
    a Virtual Machine provides functionality of a physical computer \cite{virtualizationOverview}} (i.e. VMware Workstation) 
    through which researchers could create  
    a virtual computer and set up the experiment pipeline on that. They could then, leave the experiment 
    pipeline as it is (including all the involved software, coded programs and dependencies along with the environment) 
    in the state that the results are produced. 
    The experiment could then be shared through sharing the virtual environment. So, any 
    further research on the same problem could be conducted by loading the container (or the virtual machine) and running 
    the same experiment through the same environment. This would significantly speed up the process as there would be no need for  
    replication process.



\myparagraph{Reproducible Experiment Report Checklist}
\begin{table}[ht]
    \centering
    \begin{tabular}{| P{14cm} || p{2cm} |}
        \hline
        \rowcolor{gray}\multicolumn{2}{|L|}{Data Provenance and Sharing} \\
        \hline
            \begin{itemize}
                \item
                {\small For the curation process:}
                    \begin{itemize}
                            \item
                            {\footnotesize Report on the source(s) where the data is taken from 
                            (for sequences, report on the database, accession number and the version of each sequence.)}
                           \item
                            {\footnotesize Explain the process(es) by which the final data for the dataset was produced}
                    \end{itemize}
                \item
                {\small  Share the final dataset and provide a link to downloadable version of the dataset.}
            \end{itemize} &\\
        \hline \hline
        \rowcolor{gray}\multicolumn{2}{|l|}{Feature Provenance and Sharing}\\
        \hline
        \begin{itemize}
                \item
                {\small For each feature:}
                    \begin{itemize}
                            \item
                            {\footnotesize Explain the concept associated with the extracted feature }
                            \item
                            {\footnotesize Explain over the process through which the feature is extracted}
                           \item
                            {\footnotesize For formulas, describe the associated parameters}
                            \item
                            {\footnotesize Share the extracted feature file. If sharing the entire generated result is not feasible, then, 
                            share reasonable amount of rows from the final extracted feature file}
                    \end{itemize}
                
            \end{itemize} &\\
        \hline \hline
        \rowcolor{gray}\multicolumn{2}{|l|}{Model Provenance and Sharing}\\
        \hline
        \multicolumn{2}{|L|}{Data Pre-processing}\\ 
        \hline
            \begin{itemize}
                    \item
                    {\small For any technique being applied to the original data in dataset:}
                        \begin{itemize}
                            \item
                            {\footnotesize Explain over the technique concept or provide reference to an available resource}
                            \item
                            {\footnotesize Explain over the technique details including any involved formulas and its parameter(s)}
                            \item
                            {\footnotesize Explain how it has been applied to the original data}
                            \item
                            {\footnotesize Sharing the final transformed feature set is strongly advised.}
                        \end{itemize}
            \end{itemize}&\\
        \hline
        \multicolumn{2}{|L|}{Algorithm}\\ 
        \hline
            \begin{itemize}
                \item
                {\small Explain over any applied sampling technique along with process and involved parameter(s) }
                \item
                {\small Describe the strategy for splitting the original data into train, validation and test subsets and all 
                the related concept, process(s) and parameter(s)}
                \item
                {\small Explain the choice of the algorithm, considered range of hyper-parameters and the associated values for 
                producing the the published results}
                \item
                {\small If specific optimal hyper-parameters search technique is used, then describe the method, 
                any involved parameter(s), the process and how it has been applied to the project's data}
                \item
                {\small If dealing with multiple classes, describe the process and how the results are aggregated}
                \item
                {\small For ensemble models, report on the structure, underlying models and and the aggregation strategy. Also,
                for each involved model, describe the details mentioned above.}
                \item
                {\small Sharing the results (where possible) is strongly advised}
            \end{itemize}&\\
    
        \hline
        \multicolumn{2}{|L|}{Model Evaluation}\\ 
        \hline
            \begin{itemize}
                \item
                {\small Describe the choice of statistical method used for results' evaluation}
                \item
                {\small If averaging through multiple results, describe the methodology (micro vs macro)}
                \item
                {\small Define error bars (if any)}
            \end{itemize}&\\
        \hline

    \end{tabular}
    \captionsetup{font=small,width=12cm}
    \caption{Reproducible experiment report checklist}
    \label{tab:table3}
    
\end{table}

\begin{table}[ht]
    \centering
    \begin{tabular}{| P{14cm} || p{2cm} |}
        \hline
        \rowcolor{gray}\multicolumn{2}{|l|}{Software Provenance and Sharing}\\
        \hline
            \begin{itemize}
                \item
                {\small For coded programs, report on programming language(s), version, libraries and any required deployment instructions.}
                \item 
                {\small For any involved software, report on software details (i.e. name, version, installation process) and a reference
                to the software documentation} 
                \item
                {\small If your pipeline is environment-dependent, then, provide details over the underlying infrastructure 
                and how to setup the pipeline through that environment.}
            \end{itemize} &\\
        \hline \hline
        \rowcolor{gray}\multicolumn{2}{|l|}{Pipeline Provenance and Sharing}\\
        \hline
            \begin{itemize}
                \item
                {\small Design and code your pipeline as a chain of independently runnable modules (i.e. a module for feature extraction) 
                in a way that the output from each phase would be the input to the next one.}
                
                \item
                {\small Create a Jupyter Notebook (or any available alternative) to walk the reader through the pipeline 
                starting from the feature extraction all the way to result generation. for each step:}
                    \begin{itemize}
                        \item
                        {\footnotesize Explain briefly what the code does (or make a reference to the correspondent part in the paper) 
                        followed by a runnable code cell demonstrating how to run the code.}
                       \item
                        {\footnotesize If running the entire code for a part (i.e. feature selection) is not feasible, then leave the runnable 
                        code cell there and explain why it is not feasible to run it through the notebook. Also, explain over the expected output 
                        from this process (if available, you can load and display the process result instead)}
                \end{itemize}
                \item
                {\small If a random function is used anywhere throughout the whole process, then, 
                report on random function seed and the library details}
                
                \item
                {\small Report on any involved coded program, software or environment-related parameters, 
                  according to section \ref{sec:softwareProvenance} guidelines. }
                
                \item
                {\small Version control and share the entire project (including the output from each phase)}
                \item
                
                {\small If possible, setup your pipeline on a container (or a virtual machine) and share the entire virtual environment in 
                the state that the results are produced.}
                
            
            \end{itemize} &\\
        \hline

    \end{tabular}
    \captionsetup{font=small,width=12cm}
    \caption{Reproducible experiment report checklist (continued)}
    \label{tab:table3}
    
\end{table}