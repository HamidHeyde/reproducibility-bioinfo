\section {Conclusion}

\tbm{Whats is the best model?}
\tbm{add links to the points mentioned in table, being implemented in our paper}

Through this work, we conducted a reproducibility study on a machine learning classification problem dealing with multiple imbalanced 
classes of data. The purpose of this study is to observe and address the missing information that could affect the replication process  
through different phases of a common machine learning problem. In section \ref{sec:materials}, we addressed various scenarios that could 
be assumed regarding the available flexibility in \cite{mishra2014prediction} for model parameters. In section \ref{sec:results}, we 
provided the results from all the different scenarios mentioned above and the distance of performance metrics of each model 
from the original ones. In section \ref{sec:discussion}, we discussed possible resources 
through which these sort of problems could be handled when dealing with reproducibility. 
Through this work, we tried to implement our concerns in a project to provide a tangible reference for what we discussed in this paper.\\ 

In this section, we provide the general guidelines by which we believe one can create a reproducible work when dealing with such a problem. 
According to W3C Incubator Group Report \cite{w3c}, provenance of a resource is a record that describes entities and processes involved 
in producing and delivering or otherwise influencing that resource. 
Provenance provides a critical foundation for assessing authenticity, enabling trust, 
and allowing reproducibility. So, for addressing the human-readable provenance problems (as opposed to machine-readable provenances), 
in this study, we organized our proposed guidelines under data provenance, feature provenance, model provenance, software provenance 
and pipeline provenance. We also provided a table containing all the notes from each category along with a sample for each point 
(where possible).\\ 

\subsection{Data Provenance and Sharing}
    Data Provenance describes entities and processes involved in producing and delivering or otherwise influencing 
    that resource \cite{w3c}. It includes all the details and processes involved in putting together a dataset from the raw data. 
    We should make sure to report on the source(s) where the data (i.e. protein sequences) is taken from and the process(es) 
    by which the final data for the dataset was produced (from the original data). \\
    
    If the study involves working on dataset of sequences, then, note that a sequence receive get updated several times through years. 
    So, for each sequence, the details should include the database it has been taken from (i.e. UniProt), 
    the accession number and the version. Also if any software has been involved in the curation
    (the organization and integration of data collected from various sources) process, 
    the details should be reported as mentioned in \ref{sec:softwareProvenance}.\\
    
    A better practice would be sharing the dataset as it saves significant amount of time when trying to replicate the whole experiment.
    Providing details over the curation process along with sharing the dataset would further allow the scientists to contribute to the 
    dataset by adding new sequences. The more sequences available for a problem, there would be a greater chance to develop a model 
    that could improve the stability and accuracy of the results.
    
\subsection{Feature Provenance and Sharing}
    Feature provenance refers to the historical record of how a feature is generated from the dataset. For each feature, explain 
    over the feature you are trying to extract from the dataset (i.e. Di-peptide Composition), the process details (through which the 
    feature is generated) and expected result from this process. For formulas, a clear description of the involved parameters should be 
    included in the report. If anywhere throughout the whole process, use of a random function is required, random function seed along 
    with the library details should be included. 
    Any involved coded program, software or environment related parameters should be reported as mentioned in 
    (\ref{sec:softwareProvenance}). Sharing the generated feature in any format is strongly advised.\\
    
    Describing the feature generation process along with sharing the final feature file (or reasonable amount of the 
    produced feature set) or providing a reference to any available resource(s) (online or downloadable programs) 
    that could produce the same numerical values, would be the preferable practice as this would allow tuning the replicated 
    pipeline to the state in which the generated results would match the original ones. \\
    
    Note that some tools (being used throughout the project pipeline at the time of the experiment) might become unavailable through years. 
    Following this practice allows use of any alternative tool available at the time of the study replication (if the generated results are 
    the same as the ones being reported on the original report). Sharing the extracted feature also enables researchers to update the existing 
    program (if needed) as the updated codes could be evaluated by comparing the new generated results against the original ones.
    
\subsection{Model Provenance and Sharing}
    Model provenance refers to the historical record of how the model is trained and evaluated. This includes any possible 
    data transformation (i.e. standardization), any applied feature selection or feature engineering technique, choice of 
    the algorithm along with its hyper-parameters, the trained model and the evaluation metrics.\\
    
    Throughout the data pre-processing phase, various data transformation techniques (i.e. Standardization, Normalization, etc.)
    could be applied to the original data available in the dataset. If that is the case, then the technique 
    (i.e. Standardization (Z-score scaling)) along with any involved parameters 
    (i.e. variables calculated as (V - mean of V)/s, where `s' is the standard deviation)
    should be clearly explained. The description over how it has been applied to the original data  and the expected result 
    should be also included.\\
    
    Depending on the type of study, dealing with problems such as missing data, data with high dimensionality or imbalanced classes 
    (where there are a disproportionate ratio of observations in each class) could be the case that demands for further various techniques 
    (such as feature selection, feature engineering, imputation, etc.) to be applied to the problem. In that case, explain the applied method 
    (i.e. ensemble feature selection) along with correspondent details and parameters (i.e. explain over what type of ensemble feature selection 
    has been used including all the details). Applying the method to the original dataset and the output from this phase should also be 
    discussed in details.\\
    
    
    Before fitting an algorithm, you need to split the data into train, validation and test subsets. Depending on the amount of 
    available data to the work and the goal of the study, The sampling and allocation of of those sampled data to 
    those three subsets above could be done through different techniques. Also, applying cross validation to the feature set, is a common 
    practice for this phase. For this part, the description of the technique, the process, how it has been applied to the data, along with 
    any involved parameter(s) and the expected result(s) should be included in the report.\\
    
    
    For the model, a clear explanation over the choice of the algorithm along with its considered range of hyper-parameters and 
    the associated values for producing the 
    the published results should be included. In case of involvement of any technique for finding the optimal hyper-parameters for the model
    (i.e. grid search), the description of the method, the process, how it has been applied to the project's data along with any involved 
    parameter(s) should be included. 
    If dealing with multiple classes, describe the process and how the results are aggregated.
    If the model is an ensemble of sub-models, then the model structure, the included underlying models and the aggregation 
    strategy should be clearly explained. Also, for each model, the details mentioned above should be included.\\
    

    Depending on the problem type and the purpose of the work, various metrics could be used to evaluate the results. 
    A clear definition of the statistical method used for results' evaluation should be included. 
    When a numerical metric (i.e. accuracy) is calculated by averaging through multiple results, 
    then explain over the choice of averaging method (micro versus macro) and 
    any possible details involved with process. Also, Error bars should be clearly defined and reported (if any).\\
  
    
    If a random function is used anywhere throughout the whole process, then, random function seed along 
    with the library details should be included. Any involved coded program, software or environment related parameters 
    should be reported as mentioned in (\ref{sec:softwareProvenance}). Sharing the final transformed feature set is strongly advised.
    
\subsection{Software Provenance and Sharing}
    \label{sec:softwareProvenance}
    From the data curation (the organization and integration of data collected from various sources)
    and putting together a dataset, to model training and evaluation, different software and coded programs 
    could get involved in some phase of a typical Bioinformatics related problem. 
    Throughout the replication process, failing to use the same software (along with its parameters 
    being set to the same state or value at the time of the experiment) through the same environment, can also affect the final results. \\
    
    The report on this aspect should cover all the software, coded programs, libraries details being involved throughout the whole process. 
    For software, name, version, installation details along with documentation should be reported.
    For coded programs, programming language, version, libraries and any required deployment instructions should be described.
    If the pipeline (\ref{sec:PipelineProvenance}) is environment-dependent, then, It should also include the environment-related 
    details in which the pipeline has been executed.\\

    A good practice (if applicable to the project) is including a `requirement.txt' file in your repository (for Python-based projects)
    when sharing a project. Working on machine learning related studies, it’s common that you get a lot of libraries installed. 
    Following the instructions (available through documentation \cite{pipDoc}) you can export all the 
    installed packages and dependencies along with the current version in use into a file (Commonly named as "requirement.txt") in a state when
    the results are obtained.
    

\subsection{Pipeline Provenance and Sharing}
    \label{sec:PipelineProvenance}
    In software engineering, a pipeline is a chain of processing modules, in a way that, the output from each module is the 
    input of the next. The pipeline should be designed and coded on a modular-basis. For example, if you have a process for data cleaning, 
    feature selection or feature extraction, each one of those should be coded as an independent runnable module. The pipeline is then formed 
    by chaining all the required modules for the problem. Following this practice, if one runs through some difficulties during the replication process, 
    then there would be a greater chance for tracing and recovering from a problem associated with one piece instead of the 
    whole pipeline.\\
    
    Changing direction in an experiment or setting new targets for the study (while working on the same problem) is a common phenomenon that 
    can take place throughout any research. So, in General the entire pipeline (including all chained modules) should be reported, 
    version-controlled and shared through available means (i.e GitHub). For each module, the input to the piece, expected output, 
    the module description and any possible associated parameters should be reported. 
    Creating a walk-through guidelines for the pipeline is considered a better practice as it would walk the reader through the entire project 
    providing a brief explanation over how the whole process works.\\
    
    The common popular practice for this purpose is Notebook creation and sharing. Using applications like "Jupyter Notebook", one can create a 
    document containing live code, equations, visualizations and narrative text to walk the reader through the pipeline starting from 
    the feature extraction all the way to result generation.\\
    
    For each step, explain briefly over what the module does (could be also referenced to the correspondent part in the paper for further details) 
    followed by a runnable code cell demonstrating how to run the code. 
    If running the entire code for a piece (i.e. feature selection) is not feasible, then leave the runnable code cell there followed by 
    explanation over the matter. If this is the case, then, provide details over the expected output from this process 
    (if available, you can load and display the process result instead).\\
    
    Another alternative solution to this problem is the use of containers \footnote{OS-level virtualization is a technology through which various virtual 
    environments could be deployed on top of a shared operating system. In this type of virtualization, the underlying operating system is partitioned
    to create multiple isolated Virtual Machines (VM) providing the functionality of a physical computer system. \cite{osLevelVM}} (i.e. Docker) 
    or virtual machines \footnote{A virtual machine (VM) is an emulated computer system. Based on the computer architectures, 
    a Virtual Machine provides functionality of a physical computer \cite{virtualizationOverview}} (i.e. VMware Workstation) 
    through which researchers could create  
    a virtual computer and set up the experiment pipeline on that. They could then, leave the experiment 
    pipeline as it is (including all the involved software, coded programs and dependencies along with the environment) 
    in the state that the results are produced. 
    The experiment could then be shared through sharing the virtual environment. So, any 
    further research on the same problem could be conducted by loading the container (or the virtual machine) and running 
    the same experiment through the same environment. This would significantly speed up the process as there would be no need for  
    replication process.



\myparagraph{Reproducible Experiment Report Checklist}
\begin{table}[ht]
    \centering
    \begin{tabular}{| P{16cm} |}
        \hline
        \rowcolor{gray}\multicolumn{1}{|L|}{\small{Data Provenance and Sharing}}\\
        \hline
            \begin{itemize}
                \tabitem{Report on the datasource(s) (for sequences, report on the database, the accession number and the version}
                \tabitem{Explain the process(es) by which the final data for the dataset was produced}
                \tabitem{Share the final dataset (\git{tree/master/dataset/trainTest}{sample})}
            \end{itemize}\\
        \hline \hline

        \rowcolor{gray}\multicolumn{1}{|l|}{\small{Feature Provenance and Sharing}}\\
        \hline
        \begin{itemize}
                \tabitem{Explain the concept associated with the extracted feature }
                \tabitem{Explain the process through which the feature is extracted}
                \tabitem{For formulas, describe the associated parameters}
                \tabitem {Share the extracted feature file. If sharing the entire generated result is not feasible, then, 
                share reasonable amount of the final extracted feature file (\git{tree/master/features/trainTest}{sample})}
            \end{itemize}\\
        \hline \hline
        \rowcolor{gray}\multicolumn{1}{|l|}{\small{Model Provenance and Sharing}}\\
        \hline
        \multicolumn{1}{|L|}{\small{Data Pre-processing}}\\ 
        \hline
            \begin{itemize}
                \tabitem{Explain the pre-processing technique concept or provide reference to an available resource}
                \tabitem{Explain the pre-processing technique details including any involved formulas and its parameter(s)}
                \tabitem{Explain how it has been applied to the original data}
                \tabitem{Share the final transformed feature set,If sharing the entire generated result is not feasible, then, 
                share reasonable amount of rows from the final transformed feature file}
            \end{itemize}\\
        \hline
        \multicolumn{1}{|L|}{\small{Model Structure}}\\ 
        \hline
            \begin{itemize}
                \tabitem{Explain over any applied sampling technique along with process and involved parameter(s) }
                \tabitem{Describe the strategy for splitting the original data into train, validation and test subsets}
                \tabitem{Explain the deployed algorithm, considered range of hyper-parameters and the associated values for 
                obtaining the published results}
                \tabitem{If a specific optimal hyper-parameters search technique is used, provide the final deployed values 
                resulted from the process, describe the method, any involved parameter(s), the process and 
                how it has been applied to the project's data. }
                \tabitem{If dealing with multiple classes, describe the process and how the results are aggregated}
                \tabitem{For ensemble models, report on the structure, underlying models and the aggregation strategy. Also,
                for each involved model, describe the details mentioned above.}
                \tabitem{Share reasonable amount of the generated results (where possible)}
            \end{itemize}\\
    
        \hline
        \multicolumn{1}{|L|}{\small{Model Evaluation}}\\ 
        \hline
            \begin{itemize}
                \tabitem{Describe the choice of statistical method used for evaluation of the results, any involved formula 
                and its parameter(s)}
                \tabitem{If averaging through multiple results, describe the methodology (micro vs macro)}
                \tabitem{Define error bars (if any)}
            \end{itemize}\\
        \hline

    \end{tabular}
    \captionsetup{font=small,width=12cm}
    \caption{Reproducible experiment report checklist}
    \label{tab:table3}
    
\end{table}

\begin{table}[ht]
    \centering
    \begin{tabular}{| P{16cm} |}
        \hline
        \rowcolor{gray}\multicolumn{1}{|l|}{\small{Software Provenance and Sharing}}\\
        \hline
            \begin{itemize}
                \tabitem{For coded programs, report on programming language(s), version, libraries and any required installation instructions.}
                \tabitem{For any involved software, report on software details and a reference to the software documentation} 
                \tabitem{If your pipeline is environment-dependent, then, provide details over the underlying infrastructure 
                and how to setup the pipeline through that environment.}
            \end{itemize} \\
        \hline
        \rowcolor{gray}\multicolumn{1}{|l|}{\small{Pipeline Provenance and Sharing}}\\
        \hline
            \begin{itemize}
                \tabitem{Design and code your pipeline as a chain of independently runnable modules (i.e. a module for feature extraction) 
                in a way that the output from each phase would be the input to the next one.}
                
                \tabitem{Create a Jupyter Notebook (or any available alternative) to walk the reader through the pipeline 
                starting from the feature extraction all the way to result generation. for each step:}
                    \begin{itemize}
                        \tabitem{Explain briefly what the code does (or make a reference to the correspondent part in the paper) 
                        followed by a runnable code cell demonstrating how to run the code.}
                        \tabitem{If running the entire code for a part (i.e. feature selection) is not feasible, then leave the runnable 
                        code cell there and explain why it is not feasible to run it through the notebook. Also, explain over the expected output 
                        from this process (if available, you can load and display the process result instead)}
                \end{itemize}
                \tabitem{If a random function is used anywhere throughout the whole process, then, 
                report on random function seed and the library details}
                
                \tabitem{Report on any involved coded program, software or environment-related parameters, 
                  as mentioned in \ref{sec:softwareProvenance}. }
                
                \tabitem{Version control and share the entire project (including the output from each phase)}
                \tabitem{If possible, setup your pipeline on a container (or a virtual machine) and share the entire virtual environment in 
                the state that the results are produced.}
                
            
            \end{itemize} \\
        \hline

    \end{tabular}
    \captionsetup{font=small,width=12cm}
    \caption{Reproducible experiment report checklist (continued)}
    \label{tab:table3}
    
\end{table}