{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distance Function\n",
    "def distance(metrics):\n",
    "\n",
    "    original = {}\n",
    "    original['accuracy'] = 0.7374\n",
    "    original['sensitivity'] = 0.7465\n",
    "    original['specificity'] = 0.7322\n",
    "    original['mcc'] = 0.46\n",
    "\n",
    "    distance = math.pow((metrics['accuracy'] -original['accuracy']),2)\n",
    "    distance = distance + math.pow((metrics['sensitivity'] -original['sensitivity']),2)\n",
    "    distance = distance + math.pow((metrics['specificity'] -original['specificity']),2)\n",
    "    distance = distance + math.pow((metrics['mcc'] -original['mcc']),2)\n",
    "\n",
    "    distance = math.sqrt(distance)\n",
    "\n",
    "    return distance\n",
    "\n",
    "#Calculate_Metrics\n",
    "def calculate_metrics(tp,tn,fp,fn):\n",
    "    accuracy=(tp+tn)/(tp+tn+fp+fn)\n",
    "    if (tp== 0 and fn==0):    \n",
    "        sensitivity=0\n",
    "    else:\n",
    "        sensitivity=tp/(tp+fn)\n",
    "    \n",
    "    if (tn== 0 and fp==0):    \n",
    "        specificity=0\n",
    "    else:\n",
    "        specificity=tn/(tn+fp)\n",
    "        \n",
    "    \n",
    "    \n",
    "    if (tp== 0 or tn==0) and (fn==0 or fp==0):\n",
    "        mcc=0\n",
    "    else:\n",
    "        mcc= ((tp*tn)-(fn*fp))/math.sqrt((tp+fn)*(tn+fp)*(tp+fp)*(tn+fn))\n",
    "\n",
    "    metric=[accuracy,sensitivity,specificity, mcc]\n",
    "    metrics=[round(_metric*100,2) for _metric in metric]\n",
    "    return metrics\n",
    "#Metrics from confusion matrix\n",
    "def calc_from_matrix(mat):\n",
    "    \n",
    "    total=(sum(sum(mat)))\n",
    "    tps=[mat[i][i] for i in range(len(mat))]\n",
    "#     print(tps)\n",
    "\n",
    "    _fn=[]\n",
    "    fnn=[mat[i] for i in range(len(mat))]\n",
    "    for i in range(len(mat)):\n",
    "        temp=list(fnn[i])\n",
    "        del temp[i]\n",
    "        _fn.append(temp)\n",
    "\n",
    "#     print(_fn)\n",
    "    _fp=[]\n",
    "    fpp=[mat[:,i] for i in range(len(mat))]\n",
    "    for i in range(len(mat)):\n",
    "        temp=list(fpp[i])\n",
    "        del temp[i]\n",
    "        _fp.append(temp)\n",
    "\n",
    "    _fold_macro_metrics=[]\n",
    "    _fold_micro_metrics=[]\n",
    "    for i in range(len(mat)):\n",
    "        tp = tps[i]\n",
    "        fn = sum(_fn[i])\n",
    "        fp = sum(_fp[i])\n",
    "        tn = total -(tp+fp+fn)\n",
    "#         print([tp,tn,fp,fn])\n",
    "        _metrics=calculate_metrics(tp,tn,fp,fn)\n",
    "#         print(_metrics)\n",
    "        _fold_macro_metrics.append(_metrics)\n",
    "        _fold_micro_metrics.append([tp,fn,fp,tn])\n",
    "\n",
    "\n",
    "    # fold_metrics\n",
    "    # metric=[accuracy,sensitivity,specificity, mcc]\n",
    "    _fold_acc=np.sum([item[0] for item in _fold_macro_metrics]) / len(_fold_macro_metrics)\n",
    "    _fold_sens=np.sum([item[1] for item in _fold_macro_metrics]) / len(_fold_macro_metrics)\n",
    "    _fold_spec=np.sum([item[2] for item in _fold_macro_metrics]) / len(_fold_macro_metrics)\n",
    "    _fold_mcc=np.sum([item[3] for item in _fold_macro_metrics]) / len(_fold_macro_metrics)\n",
    "\n",
    "    _fold_macros=[_fold_acc,_fold_sens,_fold_spec,_fold_mcc]\n",
    "\n",
    "    _fold_tp=np.sum([item[0] for item in _fold_micro_metrics]) / len(_fold_micro_metrics)\n",
    "    _fold_fn=np.sum([item[1] for item in _fold_micro_metrics]) / len(_fold_micro_metrics)\n",
    "    _fold_fp=np.sum([item[2] for item in _fold_micro_metrics]) / len(_fold_micro_metrics)\n",
    "    _fold_tn=np.sum([item[3] for item in _fold_micro_metrics]) / len(_fold_micro_metrics)\n",
    "\n",
    "    _fold_micros=calculate_metrics(_fold_tp,_fold_tn,_fold_fp,_fold_fn)\n",
    "\n",
    "#     print(\"==================================\")\n",
    "#     print(\"Macro\")\n",
    "#     print(_fold_macros)\n",
    "#     print(\"----------------------------------\")\n",
    "#     print(\"Micro\")\n",
    "#     print(_fold_micros)\n",
    "#     print(\"==================================\")\n",
    "    \n",
    "    return [_fold_macros,_fold_micros]\n",
    "\n",
    "#Accuracy\n",
    "def _accuracy(actual, predicted):\n",
    "    return accuracy_score(actual, predicted)\n",
    "        \n",
    "\n",
    "#Sensitivity(Recall)\n",
    "def _sens(actual, predicted):\n",
    "    return precision_recall_fscore_support(actual, predicted,average='weighted')[1]\n",
    "\n",
    "#MCC\n",
    "def _mcc(actual, predicted):\n",
    "    return matthews_corrcoef(actual,predicted)\n",
    "\n",
    "#up-sampling\n",
    "def _upSample(_dataset, _size):\n",
    "    \n",
    "    # Creating the unique class names and then sorting\n",
    "    _classes=sorted(_dataset['label'].unique())\n",
    "    # Putting each class in a differnet data frame\n",
    "    _data_by_class=[_dataset[_dataset['label']==_class] for _class in _classes ]\n",
    "    \n",
    "    for ii in range(len(_data_by_class)):\n",
    "        np.random.seed(2)\n",
    "        _data_by_class[ii]=_data_by_class[ii].reindex(np.random.permutation(_data_by_class[ii].index))\n",
    "\n",
    "        if len(_data_by_class[ii])>_size:\n",
    "            _data_by_class[ii]=_data_by_class[ii][:_size][:]\n",
    "        else:\n",
    "            _time = _size // len(_data_by_class[ii])\n",
    "            _res=[]\n",
    "            if _time > 1:\n",
    "                _time=_time+1\n",
    "            for xx in range(_time):\n",
    "                _res.append(_data_by_class[ii])\n",
    "            \n",
    "            _data_by_class[ii]=pd.concat([item for item in _res], axis=0)\n",
    "            _data_by_class[ii]=_data_by_class[ii][:_size][:]\n",
    "                \n",
    "            \n",
    "    _folds_by_class_=[]\n",
    "    _num_of_folds=5\n",
    "    # Creating folds from each class\n",
    "    for _item in _data_by_class:\n",
    "        co=_size//_num_of_folds\n",
    "        _folds_by_class_.append([_item[(i*co):((i*co)+co)] for i in range(_num_of_folds)])\n",
    "    \n",
    "    # Creating Folds from the whole feature\n",
    "    # by concatenation of each fold from each class\n",
    "    _folds_=[\n",
    "        pd.concat([_folds_by_class_[i][j] for i in range(len(_folds_by_class_))]) \n",
    "        for j in range(_num_of_folds)\n",
    "        ]\n",
    "    \n",
    "    # Train,Test out of 5 folds\n",
    "    _train_test_=[]\n",
    "    for ii in range(_num_of_folds):\n",
    "        _test__ =_folds_[ii]\n",
    "        _train__=pd.concat([_folds_[xx] for xx in range(_num_of_folds) if xx!=ii])\n",
    "        _train_test_.append([_train__,_test__])\n",
    "    \n",
    "    return _train_test_\n",
    "    \n",
    "    \n",
    "#down-Smapling Data\n",
    "def _downSample(_dataset, _size):\n",
    "\n",
    "    # Creating the unique class names and then sorting\n",
    "    _classes=sorted(_dataset['label'].unique())\n",
    "    # Putting each class in a differnet data frame\n",
    "    _data_by_class=[_dataset[_dataset['label']==_class] for _class in _classes ]\n",
    "    \n",
    "    _folds_by_class_=[]\n",
    "    _num_of_folds=5\n",
    "    # Creating folds from each class\n",
    "    for _item in _data_by_class:\n",
    "        np.random.seed(2)\n",
    "        _item=_item.reindex(np.random.permutation(_item.index))\n",
    "        co=_size//_num_of_folds\n",
    "        _folds_by_class_.append([_item[(i*co):((i*co)+co)] for i in range(_num_of_folds)])\n",
    "    \n",
    "    # Creating Folds from the whole feature\n",
    "    # by concatenation of each fold from each class\n",
    "    _folds_=[\n",
    "        pd.concat([_folds_by_class_[i][j] for i in range(len(_folds_by_class_))]) \n",
    "        for j in range(_num_of_folds)\n",
    "        ]\n",
    "    \n",
    "    # Train,Test out of 5 folds\n",
    "    _train_test_=[]\n",
    "    for ii in range(_num_of_folds):\n",
    "        _test__ =_folds_[ii]\n",
    "        _train__=pd.concat([_folds_[xx] for xx in range(_num_of_folds) if xx!=ii])\n",
    "        _train_test_.append([_train__,_test__])\n",
    "    \n",
    "    return _train_test_\n",
    "\n",
    "#Splits Data into [X_train, X_test, y_train, y_test]\n",
    "def _split_(_train,_test):\n",
    "    \n",
    "    y_test = _test['label'].values\n",
    "    del _test['label']\n",
    "    X_test = _test.values\n",
    "    \n",
    "    y_train = _train['label'].values\n",
    "    del _train['label']\n",
    "    X_train = _train.values\n",
    "\n",
    "    \n",
    "    return [X_train, X_test, y_train, y_test]\n",
    "\n",
    "#Prediciton\n",
    "def _predict(_train_test,_g,_c,_type):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = _train_test\n",
    "    \n",
    "    _gamma,_C =_g,_c\n",
    "    \n",
    "    classifier = None\n",
    "    if _type=='svc':\n",
    "        classifier=SVC(kernel='rbf', gamma=_gamma,C=_C, decision_function_shape='ovr')    \n",
    "    elif _type=='onevsall':\n",
    "        classifier = OneVsRestClassifier( SVC(kernel='rbf', gamma=_gamma,C=_C) )\n",
    "        \n",
    "\n",
    "    #Training the algorithm on training data\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predicting the Test set results\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    return [y_test.copy(),y_pred.copy()]\n",
    "\n",
    "#Compute from confucion matrix\n",
    "def computeConfusionMatrix(_res):\n",
    "    all_macros_acc=np.sum([item[0][0] for item in _res]) / len(_res)\n",
    "    all_macros_sens=np.sum([item[0][1] for item in _res]) / len(_res)\n",
    "    all_macros_spec=np.sum([item[0][2] for item in _res]) / len(_res)\n",
    "    all_macros_mcc=np.sum([item[0][3] for item in _res]) / len(_res)\n",
    "\n",
    "    _problem_macros=[all_macros_acc,all_macros_sens,all_macros_spec,all_macros_mcc]\n",
    "    _mac_distance = distance({\n",
    "        'accuracy':(all_macros_acc/100),\n",
    "        'sensitivity':(all_macros_sens/100),\n",
    "        'specificity':(all_macros_spec/100),\n",
    "        'mcc':all_macros_mcc/100\n",
    "        })\n",
    "\n",
    "    all_micros_acc=np.sum([item[1][0] for item in _res]) / len(_res)\n",
    "    all_micros_sens=np.sum([item[1][1] for item in _res]) / len(_res)\n",
    "    all_micros_spec=np.sum([item[1][2] for item in _res]) / len(_res)\n",
    "    all_micros_mcc=np.sum([item[1][3] for item in _res]) / len(_res)\n",
    "\n",
    "    _problem_micros=[all_micros_acc,all_micros_sens,all_micros_spec,all_micros_mcc]\n",
    "    _mic_distance = distance({\n",
    "        'accuracy':(all_micros_acc/100),\n",
    "        'sensitivity':(all_micros_sens/100),\n",
    "        'specificity':(all_micros_spec/100),\n",
    "        'mcc':all_micros_mcc/100\n",
    "        })\n",
    "\n",
    "    print(\"MACRO========================\")\n",
    "    print(_problem_macros)\n",
    "    print(_mac_distance)\n",
    "\n",
    "    print(\"MICRO========================\")\n",
    "    print(_problem_micros)\n",
    "    print(_mic_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data7=pd.read_csv(os.path.join('dataset','aac7.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data7['label'].values\n",
    "le=LabelEncoder()\n",
    "y=le.fit_transform(y)\n",
    "data7['label']=y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results (7 classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On Sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "_res_svc=[]\n",
    "_res_onvsall=[]\n",
    "for train, test in kf.split(data7.copy()):\n",
    "\n",
    "    df_train=pd.DataFrame(data7.iloc[train])\n",
    "    df_test=pd.DataFrame(data7.iloc[test])\n",
    "    #SVC\n",
    "    y7_test,y7_pred=_predict(_split_(df_train.copy(),df_test.copy()).copy(),0.02,4.6,'svc')\n",
    "    _mat = confusion_matrix(y7_test.copy(),y7_pred.copy())\n",
    "    _res_svc.append(calc_from_matrix(_mat.copy()))\n",
    "    #onvsall\n",
    "    y7_test,y7_pred=_predict(_split_(df_train.copy(),df_test.copy()).copy(),0.02,4.6,'onevsall')\n",
    "    _mat = confusion_matrix(y7_test.copy(),y7_pred.copy())\n",
    "    _res_onvsall.append(calc_from_matrix(_mat.copy()))\n",
    "\n",
    "#SVC\n",
    "print(\"SVC results =========\")\n",
    "computeConfusionMatrix(_res_svc.copy())\n",
    "#SVC\n",
    "print(\"ONE vs All results =========\")\n",
    "computeConfusionMatrix(_res_onvsall.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On Shuffled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "np.random.seed(2)\n",
    "data7s=data7.copy()\n",
    "data7s=data7s.reindex(np.random.permutation(data7s.index))\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "_res_svc=[]\n",
    "_res_onvsall=[]\n",
    "\n",
    "for train, test in kf.split(data7s.copy()):\n",
    "    \n",
    "    df_train=pd.DataFrame(data7s.iloc[train])\n",
    "    df_test=pd.DataFrame(data7s.iloc[test])\n",
    "    #SVC\n",
    "    y7_test,y7_pred=_predict(_split_(df_train.copy(),df_test.copy()).copy(),0.02,4.6,'svc')\n",
    "    _mat = confusion_matrix(y7_test.copy(),y7_pred.copy())\n",
    "    _res_svc.append(calc_from_matrix(_mat.copy()))\n",
    "    #onvsall\n",
    "    y7_test,y7_pred=_predict(_split_(df_train.copy(),df_test.copy()).copy(),0.02,4.6,'onevsall')\n",
    "    _mat = confusion_matrix(y7_test.copy(),y7_pred.copy())\n",
    "    _res_onvsall.append(calc_from_matrix(_mat.copy()))\n",
    "    \n",
    "\n",
    "#SVC\n",
    "print(\"SVC results =========\")\n",
    "computeConfusionMatrix(_res_svc.copy())\n",
    "#SVC\n",
    "print(\"ONE vs All results =========\")\n",
    "computeConfusionMatrix(_res_onvsall.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On Down Sampled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_test__ = _downSample(data7.copy(),60)\n",
    "_res_svc=[]\n",
    "_res_onvsall=[]\n",
    "for train, test in _train_test__:\n",
    "\n",
    "    #y7_test__,y7_pred__=_predict(_split_(train.copy(),test.copy()),0.02,4.6)\n",
    "    #SVC\n",
    "    y7_test,y7_pred=_predict(_split_(train.copy(),test.copy()).copy(),0.02,4.6,'svc')\n",
    "    _mat = confusion_matrix(y7_test.copy(),y7_pred.copy())\n",
    "    _res_svc.append(calc_from_matrix(_mat.copy()))\n",
    "    #onvsall\n",
    "    y7_test,y7_pred=_predict(_split_(train.copy(),test.copy()).copy(),0.02,4.6,'onevsall')\n",
    "    _mat = confusion_matrix(y7_test.copy(),y7_pred.copy())\n",
    "    _res_onvsall.append(calc_from_matrix(_mat.copy()))\n",
    "\n",
    "#SVC\n",
    "print(\"SVC results =========\")\n",
    "computeConfusionMatrix(_res_svc.copy())\n",
    "#SVC\n",
    "print(\"ONE vs All results =========\")\n",
    "computeConfusionMatrix(_res_onvsall.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data8=pd.read_csv(os.path.join('dataset','aac8.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data8['label'].values\n",
    "le=LabelEncoder()\n",
    "y=le.fit_transform(y)\n",
    "data8['label']=y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results 8-classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On Sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "_res_svc=[]\n",
    "_res_onvsall=[]\n",
    "for train, test in kf.split(data8.copy()):\n",
    "\n",
    "    df_train=pd.DataFrame(data8.iloc[train])\n",
    "    df_test=pd.DataFrame(data8.iloc[test])\n",
    "    #SVC\n",
    "    y7_test,y7_pred=_predict(_split_(df_train.copy(),df_test.copy()).copy(),0.02,4.1,'svc')\n",
    "    _mat = confusion_matrix(y7_test.copy(),y7_pred.copy())\n",
    "    _res_svc.append(calc_from_matrix(_mat.copy()))\n",
    "    #onvsall\n",
    "    y7_test,y7_pred=_predict(_split_(df_train.copy(),df_test.copy()).copy(),0.02,4.1,'onevsall')\n",
    "    _mat = confusion_matrix(y7_test.copy(),y7_pred.copy())\n",
    "    _res_onvsall.append(calc_from_matrix(_mat.copy()))\n",
    "\n",
    "#SVC\n",
    "print(\"SVC results =========\")\n",
    "computeConfusionMatrix(_res_svc.copy())\n",
    "#SVC\n",
    "print(\"ONE vs All results =========\")\n",
    "computeConfusionMatrix(_res_onvsall.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On Shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "np.random.seed(2)\n",
    "data8s=data8.copy()\n",
    "data8s=data8s.reindex(np.random.permutation(data8s.index))\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "_res_svc=[]\n",
    "_res_onvsall=[]\n",
    "\n",
    "for train, test in kf.split(data8s.copy()):\n",
    "    \n",
    "    df_train=pd.DataFrame(data8s.iloc[train])\n",
    "    df_test=pd.DataFrame(data8s.iloc[test])\n",
    "    #SVC\n",
    "    y7_test,y7_pred=_predict(_split_(df_train.copy(),df_test.copy()).copy(),0.02,4.1,'svc')\n",
    "    _mat = confusion_matrix(y7_test.copy(),y7_pred.copy())\n",
    "    _res_svc.append(calc_from_matrix(_mat.copy()))\n",
    "    #onvsall\n",
    "    y7_test,y7_pred=_predict(_split_(df_train.copy(),df_test.copy()).copy(),0.02,4.1,'onevsall')\n",
    "    _mat = confusion_matrix(y7_test.copy(),y7_pred.copy())\n",
    "    _res_onvsall.append(calc_from_matrix(_mat.copy()))\n",
    "    \n",
    "\n",
    "#SVC\n",
    "print(\"SVC results =========\")\n",
    "computeConfusionMatrix(_res_svc.copy())\n",
    "#SVC\n",
    "print(\"ONE vs All results =========\")\n",
    "computeConfusionMatrix(_res_onvsall.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_test__ = _downSample(data8.copy(),60)\n",
    "_res_svc=[]\n",
    "_res_onvsall=[]\n",
    "for train, test in _train_test__:\n",
    "\n",
    "    #y7_test__,y7_pred__=_predict(_split_(train.copy(),test.copy()),0.02,4.6)\n",
    "    #SVC\n",
    "    y7_test,y7_pred=_predict(_split_(train.copy(),test.copy()).copy(),0.02,4.6,'svc')\n",
    "    _mat = confusion_matrix(y7_test.copy(),y7_pred.copy())\n",
    "    _res_svc.append(calc_from_matrix(_mat.copy()))\n",
    "    #onvsall\n",
    "    y7_test,y7_pred=_predict(_split_(train.copy(),test.copy()).copy(),0.02,4.6,'onevsall')\n",
    "    _mat = confusion_matrix(y7_test.copy(),y7_pred.copy())\n",
    "    _res_onvsall.append(calc_from_matrix(_mat.copy()))\n",
    "\n",
    "#SVC\n",
    "print(\"SVC results =========\")\n",
    "computeConfusionMatrix(_res_svc.copy())\n",
    "#SVC\n",
    "print(\"ONE vs All results =========\")\n",
    "computeConfusionMatrix(_res_onvsall.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
