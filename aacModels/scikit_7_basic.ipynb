{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy\n",
    "def _accuracy(actual, predicted):\n",
    "    return accuracy_score(actual, predicted)\n",
    "        \n",
    "\n",
    "#Sensitivity(Recall)\n",
    "def _sens(actual, predicted):\n",
    "    return precision_recall_fscore_support(actual, predicted,average='weighted')[1]\n",
    "\n",
    "#MCC\n",
    "def _mcc(actual, predicted):\n",
    "    return matthews_corrcoef(actual,predicted)\n",
    "\n",
    "#up-sampling\n",
    "def _upSample(_dataset, _size):\n",
    "    \n",
    "    # Creating the unique class names and then sorting\n",
    "    _classes=sorted(_dataset['label'].unique())\n",
    "    # Putting each class in a differnet data frame\n",
    "    _data_by_class=[_dataset[_dataset['label']==_class] for _class in _classes ]\n",
    "    \n",
    "    for ii in range(len(_data_by_class)):\n",
    "        np.random.seed(2)\n",
    "        _data_by_class[ii]=_data_by_class[ii].reindex(np.random.permutation(_data_by_class[ii].index))\n",
    "\n",
    "        if len(_data_by_class[ii])>_size:\n",
    "            _data_by_class[ii]=_data_by_class[ii][:_size][:]\n",
    "        else:\n",
    "            _time = _size // len(_data_by_class[ii])\n",
    "            _res=[]\n",
    "            if _time > 1:\n",
    "                _time=_time+1\n",
    "            for xx in range(_time):\n",
    "                _res.append(_data_by_class[ii])\n",
    "            \n",
    "            _data_by_class[ii]=pd.concat([item for item in _res], axis=0)\n",
    "            _data_by_class[ii]=_data_by_class[ii][:_size][:]\n",
    "                \n",
    "            \n",
    "    _folds_by_class_=[]\n",
    "    _num_of_folds=5\n",
    "    # Creating folds from each class\n",
    "    for _item in _data_by_class:\n",
    "        co=_size//_num_of_folds\n",
    "        _folds_by_class_.append([_item[(i*co):((i*co)+co)] for i in range(_num_of_folds)])\n",
    "    \n",
    "    # Creating Folds from the whole feature\n",
    "    # by concatenation of each fold from each class\n",
    "    _folds_=[\n",
    "        pd.concat([_folds_by_class_[i][j] for i in range(len(_folds_by_class_))]) \n",
    "        for j in range(_num_of_folds)\n",
    "        ]\n",
    "    \n",
    "    # Train,Test out of 5 folds\n",
    "    _train_test_=[]\n",
    "    for ii in range(_num_of_folds):\n",
    "        _test__ =_folds_[ii]\n",
    "        _train__=pd.concat([_folds_[xx] for xx in range(_num_of_folds) if xx!=ii])\n",
    "        _train_test_.append([_train__,_test__])\n",
    "    \n",
    "    return _train_test_\n",
    "    \n",
    "    \n",
    "#down-Smapling Data\n",
    "def _downSample(_dataset, _size):\n",
    "\n",
    "    # Creating the unique class names and then sorting\n",
    "    _classes=sorted(_dataset['label'].unique())\n",
    "    # Putting each class in a differnet data frame\n",
    "    _data_by_class=[_dataset[_dataset['label']==_class] for _class in _classes ]\n",
    "    \n",
    "    _folds_by_class_=[]\n",
    "    _num_of_folds=5\n",
    "    # Creating folds from each class\n",
    "    for _item in _data_by_class:\n",
    "        np.random.seed(2)\n",
    "        _item=_item.reindex(np.random.permutation(_item.index))\n",
    "        co=_size//_num_of_folds\n",
    "        _folds_by_class_.append([_item[(i*co):((i*co)+co)] for i in range(_num_of_folds)])\n",
    "    \n",
    "    # Creating Folds from the whole feature\n",
    "    # by concatenation of each fold from each class\n",
    "    _folds_=[\n",
    "        pd.concat([_folds_by_class_[i][j] for i in range(len(_folds_by_class_))]) \n",
    "        for j in range(_num_of_folds)\n",
    "        ]\n",
    "    \n",
    "    # Train,Test out of 5 folds\n",
    "    _train_test_=[]\n",
    "    for ii in range(_num_of_folds):\n",
    "        _test__ =_folds_[ii]\n",
    "        _train__=pd.concat([_folds_[xx] for xx in range(_num_of_folds) if xx!=ii])\n",
    "        _train_test_.append([_train__,_test__])\n",
    "    \n",
    "    return _train_test_\n",
    "\n",
    "#Splits Data into [X_train, X_test, y_train, y_test]\n",
    "def _split_(_train,_test):\n",
    "    \n",
    "    y_test = _test['label'].values\n",
    "    del _test['label']\n",
    "    X_test = _test.values\n",
    "    \n",
    "    y_train = _train['label'].values\n",
    "    del _train['label']\n",
    "    X_train = _train.values\n",
    "\n",
    "    \n",
    "    return [X_train, X_test, y_train, y_test]\n",
    "\n",
    "#Prediciton\n",
    "def _predict(_train_test,_g,_c):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = _train_test\n",
    "    \n",
    "    _gamma,_C =_g,_c\n",
    "    #classifier = OneVsRestClassifier( SVC(kernel='rbf', gamma=_gamma,C=_C) )\n",
    "    classifier=SVC(kernel='rbf', gamma=_gamma,C=_C, decision_function_shape='ovr')    \n",
    "\n",
    "    #Training the algorithm on training data\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predicting the Test set results\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    return [y_test.copy(),y_pred.copy()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data7=pd.read_csv(os.path.join('dataset','aac7.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data7['label'].values\n",
    "le=LabelEncoder()\n",
    "y=le.fit_transform(y)\n",
    "data7['label']=y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No Shuffleing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py:543: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py:543: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_acc =  0.1833333333333333\n",
      "_mcc =  0.030775864561116317\n",
      "_sens =  0.1833333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "res=[]\n",
    "for train, test in kf.split(data7.copy()):\n",
    "\n",
    "    df_train=pd.DataFrame(data7.iloc[train])\n",
    "    df_test=pd.DataFrame(data7.iloc[test])\n",
    "    y7_test,y7_pred=_predict(_split_(df_train.copy(),df_test.copy()).copy(),0.02,5.0)\n",
    "    \n",
    "    res.append([_accuracy(y7_test.copy(),y7_pred.copy()),\n",
    "                  _mcc(y7_test.copy(),y7_pred.copy()),\n",
    "                  _sens(y7_test.copy(),y7_pred.copy())\n",
    "              ])\n",
    "\n",
    "# print(aacuracy, MCC, Sensiticity)\n",
    "print('_acc = ',sum([_num[0] for _num in res])/5)\n",
    "print('_mcc = ',sum([_num[1] for _num in res])/5)\n",
    "print('_sens = ',sum([_num[2] for _num in res])/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On Shuffled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_acc =  0.44871794871794873\n",
      "_mcc =  0.2828579969464605\n",
      "_sens =  0.44871794871794873\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "data7s=data7.copy()\n",
    "data7s=data7s.reindex(np.random.permutation(data7s.index))\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "_res=[]\n",
    "for train, test in kf.split(data7s.copy()):\n",
    "    \n",
    "    df_train=pd.DataFrame(data7s.iloc[train])\n",
    "    df_test=pd.DataFrame(data7s.iloc[test])\n",
    "    y7_test,y7_pred=_predict(_split_(df_train.copy(),df_test.copy()).copy(),0.02,4.1)\n",
    "    \n",
    "    _res.append([_accuracy(y7_test.copy(),y7_pred.copy()),\n",
    "                  _mcc(y7_test.copy(),y7_pred.copy()),\n",
    "                  _sens(y7_test.copy(),y7_pred.copy())\n",
    "              ])\n",
    "\n",
    "# print(aacuracy, MCC, Sensiticity)\n",
    "print('_acc = ',sum([_num[0] for _num in _res])/5)\n",
    "print('_mcc = ',sum([_num[1] for _num in _res])/5)\n",
    "print('_sens = ',sum([_num[2] for _num in _res])/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On Down Sampled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_acc =  0.4523809523809524\n",
      "_mcc =  0.36410513166292036\n",
      "_sens =  0.4523809523809524\n"
     ]
    }
   ],
   "source": [
    "_train_test__ = _downSample(data7.copy(),60)\n",
    "_res__=[]\n",
    "for train, test in _train_test__:\n",
    "\n",
    "    y7_test__,y7_pred__=_predict(_split_(train.copy(),test.copy()),0.03,4.5)\n",
    "    \n",
    "    _res__.append([_accuracy(y7_test__.copy(),y7_pred__.copy()),\n",
    "          _mcc(y7_test__.copy(),y7_pred__.copy()),\n",
    "          _sens(y7_test__.copy(),y7_pred__.copy())\n",
    "              ])\n",
    "\n",
    "print('_acc = ',sum([_num[0] for _num in _res__])/5)\n",
    "print('_mcc = ',sum([_num[1] for _num in _res__])/5)\n",
    "print('_sens = ',sum([_num[2] for _num in _res__])/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
